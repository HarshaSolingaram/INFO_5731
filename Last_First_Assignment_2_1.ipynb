{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshaSolingaram/INFO_5731/blob/main/Last_First_Assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwncItsTMDxL",
        "outputId": "be128670-af03-46c1-ae5b-6ceb28276aa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: semanticscholar in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (8.2.3)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (1.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (1.0.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->semanticscholar) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->semanticscholar) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install semanticscholar\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from semanticscholar import SemanticScholar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPlBtZ8kuPS0",
        "outputId": "d8a0d9e2-87bd-4aa7-8dfc-84db8237273a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing query: machine learning\n",
            "Processing query: data science\n",
            "Processing query: artificial intelligence\n",
            "Processing query: information extraction\n"
          ]
        }
      ],
      "source": [
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.width', None)\n",
        "sem_sch = SemanticScholar()\n",
        "\n",
        "queries = [\"machine learning\", \"data science\", \"artificial intelligence\", \"information extraction\"]\n",
        "df_paper = pd.DataFrame(columns=['abstract'])\n",
        "total_papers = 0\n",
        "\n",
        "for query in queries:\n",
        "\n",
        "    print(f\"Processing query: {query}\")\n",
        "    count = 0\n",
        "\n",
        "    while count < 2500:\n",
        "\n",
        "        res = sem_sch.search_paper(query, fields=['abstract'])\n",
        "\n",
        "        if not res:\n",
        "            break\n",
        "\n",
        "        for paper in res:\n",
        "\n",
        "            if paper.abstract:\n",
        "                count += 1\n",
        "                total_papers += 1\n",
        "                paper_info = {'abstract': paper.abstract}\n",
        "                df_paper = df_paper.append(paper_info, ignore_index=True)\n",
        "\n",
        "            if count == 2500:\n",
        "                break\n",
        "\n",
        "df_paper.to_csv('research_papers_abstracts_2500_per_query.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5QX6bJjGWXY9",
        "outputId": "075d7b17-ad25-4e3f-c432-3968510ee6fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def remove_noise(text):\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return ' '.join([word.lower() for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "def stem_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return ' '.join([stemmer.stem(word) for word in tokens])\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "df_clean = pd.read_csv('research_papers_abstracts_2500_per_query.csv')\n",
        "\n",
        "df_clean['noise_removed'] = df_clean['abstract'].apply(remove_noise)\n",
        "df_clean['numbers_removed'] = df_clean['noise_removed'].apply(remove_numbers)\n",
        "df_clean['stopwords_removed'] = df_clean['numbers_removed'].apply(remove_stopwords)\n",
        "df_clean['lowercased'] = df_clean['stopwords_removed'].str.lower()\n",
        "df_clean['stemmed'] = df_clean['lowercased'].apply(stem_text)\n",
        "df_clean['lemmatized'] = df_clean['lowercased'].apply(lemmatize_text)\n",
        "\n",
        "df_clean.to_csv('research_papers_abstracts_cleaned_separate_steps.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XVY6ugyHN5Ml",
        "outputId": "f64f8afc-4a33-49ae-90e4-cb7ad5b0c161"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_clean\",\n  \"rows\": 9997,\n  \"fields\": [\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2992,\n        \"samples\": [\n          \"We present and evaluate a machine learning approach to constructing patient-specific classifiers that detect the onset of an epileptic seizure through analysis of the scalp EEG, a non-invasive measure of the brain's electrical activity. This problem is challenging because the brain's electrical activity is composed of numerous classes with overlapping characteristics. The key steps involved in realizing a high performance algorithm included shaping the problem into an appropriate machine learning framework, and identifying the features critical to separating seizure from other types of brain activity. When trained on 2 or more seizures per patient and tested on 916 hours of continuous EEG from 24 patients, our algorithm detected 96% of 173 test seizures with a median detection delay of 3 seconds and a median false detection rate of 2 false detections per 24 hour period. We also provide information about how to download the CHB-MIT database, which contains the data used in this study.\",\n          \"Traditional Information Extraction (IE) takes a relation name and hand-tagged examples of that relation as input. Open IE is a relationindependent extraction paradigm that is tailored to massive and heterogeneous corpora such as the Web. An Open IE system extracts a diverse set of relational tuples from text without any relation-specific input. How is Open IE possible? We analyze a sample of English sentences to demonstrate that numerous relationships are expressed using a compact set of relation-independent lexico-syntactic patterns, which can be learned by an Open IE system. What are the tradeoffs between Open IE and traditional IE? We consider this question in the context of two tasks. First, when the number of relations is massive, and the relations themselves are not pre-specified, we argue that Open IE is necessary. We then present a new model for Open IE called O-CRF and show that it achieves increased precision and nearly double the recall than the model employed by TEXTRUNNER, the previous stateof-the-art Open IE system. Second, when the number of target relations is small, and their names are known in advance, we show that O-CRF is able to match the precision of a traditional extraction system, though at substantially lower recall. Finally, we show how to combine the two types of systems into a hybrid that achieves higher precision than a traditional extractor, with comparable recall.\",\n          \"Preface. Acknowledgements. 1 Introduction. 1.1 The Distinction between Trained Sensory Panels and Consumer Panels. 1.2 The Need for Statistics in Experimental Planning and Analysis. 1.3 Scales and Data Types. 1.4 Organisation of the Book. 2 Important Data Collection Techniques for Sensory and Consumer Studies. 2.1 Sensory Panel Methodologies. 2.2 Consumer Tests. PART I PROBLEM DRIVEN. 3 Quality Control of Sensory Profile Data. 3.1 General Introduction. 3.2 Visual Inspection of Raw Data. 3.3 Mixed Model ANOVA for Assessing the Importance of the Sensory Attributes. 3.4 Overall Assessment of Assessor Differences Using All Variables Simultaneously. 3.5 Methods for Detecting Differences in Use of the Scale. 3.6 Comparing the Assessors Ability to Detect Differences between the Products. 3.7 Relations between Individual Assessor Ratings and the Panel Average. 3.8 Individual Line Plots for Detailed Inspection of Assessors. 3.9 Miscellaneous Methods.- 4 Correction Methods and Other Remedies for Improving Sensory Profile Data. 4.1 Introduction. 4.2 Correcting for Different Use of the Scale. 4.3 Computing Improved Panel Averages. 4.4 Pre-processing of Data for Three-Way Analysis. 5 Detecting and Studying Sensory Differences and Similarities between Products. 5.1 Introduction. 5.2 Analysing Sensory Profile Data: Univariate Case. 5.3 Analysing Sensory Profile Data: Multivariate Case. 6 Relating Sensory Data to Other Measurements. 6.1 Introduction. 6.2 Estimating Relations between Consensus Profiles and External Data. 6.3 Estimating Relations between Individual Sensory Profiles and External Data. 7 Discrimination and Similarity Testing. 7.1 Introduction. 7.2 Analysis of Data from Basic Sensory Discrimination Tests. 7.3 Examples of Basic Discrimination Testing. 7.4 Power Calculations in Discrimination Tests. 7.5 Thurstonian Modelling: What Is It Really? 7.6 Similarity versus Difference Testing. 7.7 Replications: What to Do? 7.8 Designed Experiments, Extended Analysis and Other Test Protocols. 8 Investigating Important Factors Influencing Food Acceptance and Choice. 8.1 Introduction. 8.2 Preliminary Analysis of Consumer Data Sets (Raw Data Overview). 8.3 Experimental Designs for Rating Based Consumer Studies. 8.4 Analysis of Categorical Effect Variables. 8.5 Incorporating Additional Information about Consumers. 8.6 Modelling of Factors as Continuous Variables. 8.7 Reliability/Validity Testing for Rating Based Methods. 8.8 Rank Based Methodology. 8.9 Choice Based Conjoint Analysis. 8.10 Market Share Simulation. 9 Preference Mapping for Understanding Relations between Sensory Product Attributes and Consumer Acceptance. 9.1 Introduction. 9.2 External and Internal Preference Mapping. 9.3 Examples of Linear Preference Mapping. 9.4 Ideal Point Preference Mapping. 9.5 Selecting Samples for Preference Mapping. 9.6 Incorporating Additional Consumer Attributes. 9.7 Combining Preference Mapping with Additional Information about the Samples. 10 Segmentation of Consumer Data. 10.1 Introduction. 10.2 Segmentation of Rating Data. 10.3 Relating Segments to Consumer Attributes. PART II METHOD ORIENTED. 11 Basic Statistics. 11.1 Basic Concepts and Principles. 11.2 Histogram, Frequency and Probability. 11.3 Some Basic Properties of a Distribution (Mean, Variance and Standard Deviation). 11.4 Hypothesis Testing and Confidence Intervals for the Mean . 11.5 Statistical Process Control. 11.6 Relationships between Two or More Variables. 11.7 Simple Linear Regression. 11.8 Binomial Distribution and Tests. 11.9 Contingency Tables and Homogeneity Testing. 12 Design of Experiments for Sensory and Consumer Data. 12.1 Introduction. 12.2 Important Concepts and Distinctions. 12.3 Full Factorial Designs. 12.4 Fractional Factorial Designs: Screening Designs. 12.5 Randomised Blocks and Incomplete Block Designs. 12.6 Split-Plot and Nested Designs. 12.7 Power of Experiments. 13 ANOVA for Sensory and Consumer Data. 13.1 Introduction. 13.2 One-Way ANOVA. 13.3 Single Replicate Two-Way ANOVA. 13.4 Two-Way ANOVA with Randomised Replications. 13.5 Multi-Way ANOVA. 13.6 ANOVA for Fractional Factorial Designs. 13.7 Fixed and Random Effects in ANOVA: Mixed Models. 13.8 Nested and Split-Plot Models. 13.9 Post Hoc Testing. 14 Principal Component Analysis. 14.1 Interpretation of Complex Data Sets by PCA. 14.2 Data Structures for the PCA. 14.3 PCA: Description of the Method. 14.4 Projections and Linear Combinations. 14.5 The Scores and Loadings Plots. 14.6 Correlation Loadings Plot. 14.7 Standardisation. 14.8 Calculations and Missing Values. 14.9 Validation. 14.10 Outlier Diagnostics. 14.11 Tucker-1. 14.12 The Relation between PCA and Factor Analysis (FA). 15 Multiple Regression, Principal Components Regression and Partial Least Squares Regression. 15.1 Introduction. 15.2 Multivariate Linear Regression. 15.3 The Relation between ANOVA and Regression Analysis. 15.4 Linear Regression Used for Estimating Polynomial Models. 15.5 Combining Continuous and Categorical Variables. 15.6 Variable Selection for Multiple Linear Regression. 15.7 Principal Components Regression (PCR). 15.8 Partial Least Squares (PLS) Regression. 15.9 Model Validation: Prediction Performance. 15.10 Model Diagnostics and Outlier Detection. 15.11 Discriminant Analysis. 15.12 Generalised Linear Models, Logistic Regression and Multinomial Regression. 16 Cluster Analysis: Unsupervised Classification. 16.1 Introduction. 16.2 Hierarchical Clustering. 16.3 Partitioning Methods. 16.4 Cluster Analysis for Matrices. 17 Miscellaneous Methodologies. 17.1 Three-Way Analysis of Sensory Data. 17.2 Relating Three-Way Data to Two-Way Data. 17.3 Path Modelling. 17.4 MDS-Multidimensional Scaling. 17.5 Analysing Rank Data. 17.6 The L-PLS Method. 17.7 Missing Value Estimation. Nomenclature, Symbols and Abbreviations. Index.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"noise_removed\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2989,\n        \"samples\": [\n          \"Earth Observation Data Cubes EODC have emerged as a promising solution to efficiently and effectively handle Big Earth Observation EO Data generated by satellites and made freely and openly available from different data repositories The aim of this Special Issue Earth Observation Data Cube in Data is to present the latest advances in EODC development and implementation including innovative approaches for the exploitation of satellite EO data using multidimensional eg spatial temporal spectral approaches This Special Issue contains  articles covering a wide range of topics such as Synthetic Aperture Radar SAR Analysis Ready Data ARD interoperability thematic applications eg land cover snow cover mapping capacity development semantics processing techniques as well as national implementations and best practices These papers made significant contributions to the advancement of a more Open and Reproducible Earth Observation Science reducing the gap between users expectations for decisionready products and current Big Data analytical capabilities and ultimately unlocking the information power of EO data by transforming them into actionable knowledge\",\n          \"The evolution from ComputerAided Instruction CAI to Intelligent ComputerAided Instruction ICAI was the first step by which education and artificial intelligence communities began to look at each others work This text looks at the evolution toward Intelligent Tutoring Systems ITS which can be thought of as a step beyond ICAI leading to more classes of problems and approaches ITS involves artificial intelligence concepts approaches dynamic student modelling human cognition intelligent user interfaces intelligent help systems and the use of strategies\",\n          \"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive noisy and incomplete data In a nutshell spectral methods refer to a collection of algorithms built upon the eigenvalues resp singular values and eigenvectors resp singular vectors of some properly designed matrices constructed from data A diverse array of applications have been found in machine learning data science and signal processing Due to their simplicity and effectiveness spectral methods are not only used as a standalone estimator but also frequently employed to initialize other more sophisticated algorithms to improve performance \\nWhile the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling with the aid of nonasymptotic random matrix theory This monograph aims to present a systematic comprehensive yet accessible introduction to spectral methods from a modern statistical perspective highlighting their algorithmic implications in diverse largescale applications In particular our exposition gravitates around several central questions that span various applications how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy and how to assess their stability in the face of random noise missing data and adversarial corruptions In addition to conventional ell perturbation analysis we present a systematic ellinfty and ellinfty perturbation theory for eigenspace and singular subspaces which has only recently become available owing to a powerful leaveoneout analysis framework\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"numbers_removed\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2989,\n        \"samples\": [\n          \"Earth Observation Data Cubes EODC have emerged as a promising solution to efficiently and effectively handle Big Earth Observation EO Data generated by satellites and made freely and openly available from different data repositories The aim of this Special Issue Earth Observation Data Cube in Data is to present the latest advances in EODC development and implementation including innovative approaches for the exploitation of satellite EO data using multidimensional eg spatial temporal spectral approaches This Special Issue contains  articles covering a wide range of topics such as Synthetic Aperture Radar SAR Analysis Ready Data ARD interoperability thematic applications eg land cover snow cover mapping capacity development semantics processing techniques as well as national implementations and best practices These papers made significant contributions to the advancement of a more Open and Reproducible Earth Observation Science reducing the gap between users expectations for decisionready products and current Big Data analytical capabilities and ultimately unlocking the information power of EO data by transforming them into actionable knowledge\",\n          \"The evolution from ComputerAided Instruction CAI to Intelligent ComputerAided Instruction ICAI was the first step by which education and artificial intelligence communities began to look at each others work This text looks at the evolution toward Intelligent Tutoring Systems ITS which can be thought of as a step beyond ICAI leading to more classes of problems and approaches ITS involves artificial intelligence concepts approaches dynamic student modelling human cognition intelligent user interfaces intelligent help systems and the use of strategies\",\n          \"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive noisy and incomplete data In a nutshell spectral methods refer to a collection of algorithms built upon the eigenvalues resp singular values and eigenvectors resp singular vectors of some properly designed matrices constructed from data A diverse array of applications have been found in machine learning data science and signal processing Due to their simplicity and effectiveness spectral methods are not only used as a standalone estimator but also frequently employed to initialize other more sophisticated algorithms to improve performance \\nWhile the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling with the aid of nonasymptotic random matrix theory This monograph aims to present a systematic comprehensive yet accessible introduction to spectral methods from a modern statistical perspective highlighting their algorithmic implications in diverse largescale applications In particular our exposition gravitates around several central questions that span various applications how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy and how to assess their stability in the face of random noise missing data and adversarial corruptions In addition to conventional ell perturbation analysis we present a systematic ellinfty and ellinfty perturbation theory for eigenspace and singular subspaces which has only recently become available owing to a powerful leaveoneout analysis framework\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stopwords_removed\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2987,\n        \"samples\": [\n          \"despite fact many companies increasing expenditure information technology obtain even sustain competitive advantage respective marketplaces many studies show benefits systems considerably less expected managers often left quandary evaluate investments realise maximum benefits reasons difficulty suggested normative literature centring around sociotechnical human organisational dimensions associated deployment inability managers determine true costs deploying considered attributable lack knowledge understanding itrelated costs benefits measurements paper discusses critical point view evaluation itis investment best practices benefits extraction investment discussion based relevant literature information ongoing research authors involving companies construction pharmaceutical computer hardware sectors\",\n          \"discovery new materials bring enormous societal technological progress context exploring completely large space potential materials computationally intractable review methods achieving inverse design aims discover tailored materials starting point particular desired functionality recent advances rapidly growing field artificial intelligence mostly subfield machine learning resulted fertile exchange ideas approaches inverse molecular design proposed employed rapid pace among deep generative models applied numerous classes materials rational design prospective drugs synthetic routes organic compounds optimization photovoltaics redox flow batteries well variety solidstate materials\",\n          \"cherenkov telescope array cta major global observatory high energy gammaray astronomy next decade beyond scientific potential cta extremely broad understanding role relativistic cosmic particles search dark matter cta explorer extreme universe probing environments immediate neighbourhood black holes cosmic voids largest scales covering huge range photon energy gev tev cta improve aspects performance respect current instruments observatory operate arrays sites hemispheres provide full sky coverage hence maximize potential rarest phenomena nearby supernovae gammaray bursts gravitational wave transients telescopes southern site telescopes northern site flexible operation possible subarrays available specific tasks cta important synergies many new generation major astronomical astroparticle observatories multiwavelength multimessenger approaches combining cta data instruments lead deeper understanding broadband nonthermal properties target sources cta observatory operated open proposaldriven observatory data available public archive predefined proprietary period scientists institutions worldwide combined together form cta consortium consortium prepared proposal core programme highly motivated observations programme encompassing approximately available observing time first ten years cta operation made individual key science projects ksps presented document\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lowercased\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2987,\n        \"samples\": [\n          \"despite fact many companies increasing expenditure information technology obtain even sustain competitive advantage respective marketplaces many studies show benefits systems considerably less expected managers often left quandary evaluate investments realise maximum benefits reasons difficulty suggested normative literature centring around sociotechnical human organisational dimensions associated deployment inability managers determine true costs deploying considered attributable lack knowledge understanding itrelated costs benefits measurements paper discusses critical point view evaluation itis investment best practices benefits extraction investment discussion based relevant literature information ongoing research authors involving companies construction pharmaceutical computer hardware sectors\",\n          \"discovery new materials bring enormous societal technological progress context exploring completely large space potential materials computationally intractable review methods achieving inverse design aims discover tailored materials starting point particular desired functionality recent advances rapidly growing field artificial intelligence mostly subfield machine learning resulted fertile exchange ideas approaches inverse molecular design proposed employed rapid pace among deep generative models applied numerous classes materials rational design prospective drugs synthetic routes organic compounds optimization photovoltaics redox flow batteries well variety solidstate materials\",\n          \"cherenkov telescope array cta major global observatory high energy gammaray astronomy next decade beyond scientific potential cta extremely broad understanding role relativistic cosmic particles search dark matter cta explorer extreme universe probing environments immediate neighbourhood black holes cosmic voids largest scales covering huge range photon energy gev tev cta improve aspects performance respect current instruments observatory operate arrays sites hemispheres provide full sky coverage hence maximize potential rarest phenomena nearby supernovae gammaray bursts gravitational wave transients telescopes southern site telescopes northern site flexible operation possible subarrays available specific tasks cta important synergies many new generation major astronomical astroparticle observatories multiwavelength multimessenger approaches combining cta data instruments lead deeper understanding broadband nonthermal properties target sources cta observatory operated open proposaldriven observatory data available public archive predefined proprietary period scientists institutions worldwide combined together form cta consortium consortium prepared proposal core programme highly motivated observations programme encompassing approximately available observing time first ten years cta operation made individual key science projects ksps presented document\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stemmed\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2987,\n        \"samples\": [\n          \"despit fact mani compani increas expenditur inform technolog obtain even sustain competit advantag respect marketplac mani studi show benefit system consider less expect manag often left quandari evalu invest realis maximum benefit reason difficulti suggest norm literatur centr around sociotechn human organis dimens associ deploy inabl manag determin true cost deploy consid attribut lack knowledg understand itrel cost benefit measur paper discuss critic point view evalu iti invest best practic benefit extract invest discuss base relev literatur inform ongo research author involv compani construct pharmaceut comput hardwar sector\",\n          \"discoveri new materi bring enorm societ technolog progress context explor complet larg space potenti materi comput intract review method achiev invers design aim discov tailor materi start point particular desir function recent advanc rapidli grow field artifici intellig mostli subfield machin learn result fertil exchang idea approach invers molecular design propos employ rapid pace among deep gener model appli numer class materi ration design prospect drug synthet rout organ compound optim photovolta redox flow batteri well varieti solidst materi\",\n          \"cherenkov telescop array cta major global observatori high energi gammaray astronomi next decad beyond scientif potenti cta extrem broad understand role relativist cosmic particl search dark matter cta explor extrem univers probe environ immedi neighbourhood black hole cosmic void largest scale cover huge rang photon energi gev tev cta improv aspect perform respect current instrument observatori oper array site hemispher provid full sky coverag henc maxim potenti rarest phenomena nearbi supernova gammaray burst gravit wave transient telescop southern site telescop northern site flexibl oper possibl subarray avail specif task cta import synergi mani new gener major astronom astroparticl observatori multiwavelength multimesseng approach combin cta data instrument lead deeper understand broadband nontherm properti target sourc cta observatori oper open proposaldriven observatori data avail public archiv predefin proprietari period scientist institut worldwid combin togeth form cta consortium consortium prepar propos core programm highli motiv observ programm encompass approxim avail observ time first ten year cta oper made individu key scienc project ksp present document\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2987,\n        \"samples\": [\n          \"despite fact many company increasing expenditure information technology obtain even sustain competitive advantage respective marketplace many study show benefit system considerably le expected manager often left quandary evaluate investment realise maximum benefit reason difficulty suggested normative literature centring around sociotechnical human organisational dimension associated deployment inability manager determine true cost deploying considered attributable lack knowledge understanding itrelated cost benefit measurement paper discus critical point view evaluation itis investment best practice benefit extraction investment discussion based relevant literature information ongoing research author involving company construction pharmaceutical computer hardware sector\",\n          \"discovery new material bring enormous societal technological progress context exploring completely large space potential material computationally intractable review method achieving inverse design aim discover tailored material starting point particular desired functionality recent advance rapidly growing field artificial intelligence mostly subfield machine learning resulted fertile exchange idea approach inverse molecular design proposed employed rapid pace among deep generative model applied numerous class material rational design prospective drug synthetic route organic compound optimization photovoltaics redox flow battery well variety solidstate material\",\n          \"cherenkov telescope array cta major global observatory high energy gammaray astronomy next decade beyond scientific potential cta extremely broad understanding role relativistic cosmic particle search dark matter cta explorer extreme universe probing environment immediate neighbourhood black hole cosmic void largest scale covering huge range photon energy gev tev cta improve aspect performance respect current instrument observatory operate array site hemisphere provide full sky coverage hence maximize potential rarest phenomenon nearby supernova gammaray burst gravitational wave transient telescope southern site telescope northern site flexible operation possible subarrays available specific task cta important synergy many new generation major astronomical astroparticle observatory multiwavelength multimessenger approach combining cta data instrument lead deeper understanding broadband nonthermal property target source cta observatory operated open proposaldriven observatory data available public archive predefined proprietary period scientist institution worldwide combined together form cta consortium consortium prepared proposal core programme highly motivated observation programme encompassing approximately available observing time first ten year cta operation made individual key science project ksps presented document\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_clean"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-9b068ae2-016f-4b78-b44e-b6b3cd81111e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>noise_removed</th>\n",
              "      <th>numbers_removed</th>\n",
              "      <th>stopwords_removed</th>\n",
              "      <th>lowercased</th>\n",
              "      <th>stemmed</th>\n",
              "      <th>lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We present Fashion-MNIST, a new dataset compri...</td>\n",
              "      <td>We present FashionMNIST a new dataset comprisi...</td>\n",
              "      <td>We present FashionMNIST a new dataset comprisi...</td>\n",
              "      <td>present fashionmnist new dataset comprising x ...</td>\n",
              "      <td>present fashionmnist new dataset comprising x ...</td>\n",
              "      <td>present fashionmnist new dataset compris x gra...</td>\n",
              "      <td>present fashionmnist new dataset comprising x ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TensorFlow is a machine learning system that o...</td>\n",
              "      <td>TensorFlow is a machine learning system that o...</td>\n",
              "      <td>TensorFlow is a machine learning system that o...</td>\n",
              "      <td>tensorflow machine learning system operates la...</td>\n",
              "      <td>tensorflow machine learning system operates la...</td>\n",
              "      <td>tensorflow machin learn system oper larg scale...</td>\n",
              "      <td>tensorflow machine learning system operates la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TensorFlow is an interface for expressing mach...</td>\n",
              "      <td>TensorFlow is an interface for expressing mach...</td>\n",
              "      <td>TensorFlow is an interface for expressing mach...</td>\n",
              "      <td>tensorflow interface expressing machine learni...</td>\n",
              "      <td>tensorflow interface expressing machine learni...</td>\n",
              "      <td>tensorflow interfac express machin learn algor...</td>\n",
              "      <td>tensorflow interface expressing machine learni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The goal of precipitation nowcasting is to pre...</td>\n",
              "      <td>The goal of precipitation nowcasting is to pre...</td>\n",
              "      <td>The goal of precipitation nowcasting is to pre...</td>\n",
              "      <td>goal precipitation nowcasting predict future r...</td>\n",
              "      <td>goal precipitation nowcasting predict future r...</td>\n",
              "      <td>goal precipit nowcast predict futur rainfal in...</td>\n",
              "      <td>goal precipitation nowcasting predict future r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Machine learning addresses the question of how...</td>\n",
              "      <td>Machine learning addresses the question of how...</td>\n",
              "      <td>Machine learning addresses the question of how...</td>\n",
              "      <td>machine learning addresses question build comp...</td>\n",
              "      <td>machine learning addresses question build comp...</td>\n",
              "      <td>machin learn address question build comput imp...</td>\n",
              "      <td>machine learning address question build comput...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9992</th>\n",
              "      <td>Most modern Information Extraction (IE) system...</td>\n",
              "      <td>Most modern Information Extraction IE systems ...</td>\n",
              "      <td>Most modern Information Extraction IE systems ...</td>\n",
              "      <td>modern information extraction ie systems imple...</td>\n",
              "      <td>modern information extraction ie systems imple...</td>\n",
              "      <td>modern inform extract ie system implement sequ...</td>\n",
              "      <td>modern information extraction ie system implem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9993</th>\n",
              "      <td>Documents contain information that can be used...</td>\n",
              "      <td>Documents contain information that can be used...</td>\n",
              "      <td>Documents contain information that can be used...</td>\n",
              "      <td>documents contain information used various app...</td>\n",
              "      <td>documents contain information used various app...</td>\n",
              "      <td>document contain inform use variou applic ques...</td>\n",
              "      <td>document contain information used various appl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9994</th>\n",
              "      <td>We develop CALM, a coordination analyzer that ...</td>\n",
              "      <td>We develop CALM a coordination analyzer that i...</td>\n",
              "      <td>We develop CALM a coordination analyzer that i...</td>\n",
              "      <td>develop calm coordination analyzer improves up...</td>\n",
              "      <td>develop calm coordination analyzer improves up...</td>\n",
              "      <td>develop calm coordin analyz improv upon conjun...</td>\n",
              "      <td>develop calm coordination analyzer improves up...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>The goal of Open Information Extraction (OIE) ...</td>\n",
              "      <td>The goal of Open Information Extraction OIE is...</td>\n",
              "      <td>The goal of Open Information Extraction OIE is...</td>\n",
              "      <td>goal open information extraction oie extract s...</td>\n",
              "      <td>goal open information extraction oie extract s...</td>\n",
              "      <td>goal open inform extract oie extract surfac re...</td>\n",
              "      <td>goal open information extraction oie extract s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>While there has been substantial progress in f...</td>\n",
              "      <td>While there has been substantial progress in f...</td>\n",
              "      <td>While there has been substantial progress in f...</td>\n",
              "      <td>substantial progress factoid questionanswering...</td>\n",
              "      <td>substantial progress factoid questionanswering...</td>\n",
              "      <td>substanti progress factoid questionansw qa ans...</td>\n",
              "      <td>substantial progress factoid questionanswering...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9997 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b068ae2-016f-4b78-b44e-b6b3cd81111e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9b068ae2-016f-4b78-b44e-b6b3cd81111e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9b068ae2-016f-4b78-b44e-b6b3cd81111e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0813fd92-44e6-4e2a-9c15-23a67bd880aa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0813fd92-44e6-4e2a-9c15-23a67bd880aa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0813fd92-44e6-4e2a-9c15-23a67bd880aa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_e50fbc1d-4d9d-4702-aa62-775f83b6a8fe\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_clean')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e50fbc1d-4d9d-4702-aa62-775f83b6a8fe button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_clean');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               abstract  \\\n",
              "0     We present Fashion-MNIST, a new dataset compri...   \n",
              "1     TensorFlow is a machine learning system that o...   \n",
              "2     TensorFlow is an interface for expressing mach...   \n",
              "3     The goal of precipitation nowcasting is to pre...   \n",
              "4     Machine learning addresses the question of how...   \n",
              "...                                                 ...   \n",
              "9992  Most modern Information Extraction (IE) system...   \n",
              "9993  Documents contain information that can be used...   \n",
              "9994  We develop CALM, a coordination analyzer that ...   \n",
              "9995  The goal of Open Information Extraction (OIE) ...   \n",
              "9996  While there has been substantial progress in f...   \n",
              "\n",
              "                                          noise_removed  \\\n",
              "0     We present FashionMNIST a new dataset comprisi...   \n",
              "1     TensorFlow is a machine learning system that o...   \n",
              "2     TensorFlow is an interface for expressing mach...   \n",
              "3     The goal of precipitation nowcasting is to pre...   \n",
              "4     Machine learning addresses the question of how...   \n",
              "...                                                 ...   \n",
              "9992  Most modern Information Extraction IE systems ...   \n",
              "9993  Documents contain information that can be used...   \n",
              "9994  We develop CALM a coordination analyzer that i...   \n",
              "9995  The goal of Open Information Extraction OIE is...   \n",
              "9996  While there has been substantial progress in f...   \n",
              "\n",
              "                                        numbers_removed  \\\n",
              "0     We present FashionMNIST a new dataset comprisi...   \n",
              "1     TensorFlow is a machine learning system that o...   \n",
              "2     TensorFlow is an interface for expressing mach...   \n",
              "3     The goal of precipitation nowcasting is to pre...   \n",
              "4     Machine learning addresses the question of how...   \n",
              "...                                                 ...   \n",
              "9992  Most modern Information Extraction IE systems ...   \n",
              "9993  Documents contain information that can be used...   \n",
              "9994  We develop CALM a coordination analyzer that i...   \n",
              "9995  The goal of Open Information Extraction OIE is...   \n",
              "9996  While there has been substantial progress in f...   \n",
              "\n",
              "                                      stopwords_removed  \\\n",
              "0     present fashionmnist new dataset comprising x ...   \n",
              "1     tensorflow machine learning system operates la...   \n",
              "2     tensorflow interface expressing machine learni...   \n",
              "3     goal precipitation nowcasting predict future r...   \n",
              "4     machine learning addresses question build comp...   \n",
              "...                                                 ...   \n",
              "9992  modern information extraction ie systems imple...   \n",
              "9993  documents contain information used various app...   \n",
              "9994  develop calm coordination analyzer improves up...   \n",
              "9995  goal open information extraction oie extract s...   \n",
              "9996  substantial progress factoid questionanswering...   \n",
              "\n",
              "                                             lowercased  \\\n",
              "0     present fashionmnist new dataset comprising x ...   \n",
              "1     tensorflow machine learning system operates la...   \n",
              "2     tensorflow interface expressing machine learni...   \n",
              "3     goal precipitation nowcasting predict future r...   \n",
              "4     machine learning addresses question build comp...   \n",
              "...                                                 ...   \n",
              "9992  modern information extraction ie systems imple...   \n",
              "9993  documents contain information used various app...   \n",
              "9994  develop calm coordination analyzer improves up...   \n",
              "9995  goal open information extraction oie extract s...   \n",
              "9996  substantial progress factoid questionanswering...   \n",
              "\n",
              "                                                stemmed  \\\n",
              "0     present fashionmnist new dataset compris x gra...   \n",
              "1     tensorflow machin learn system oper larg scale...   \n",
              "2     tensorflow interfac express machin learn algor...   \n",
              "3     goal precipit nowcast predict futur rainfal in...   \n",
              "4     machin learn address question build comput imp...   \n",
              "...                                                 ...   \n",
              "9992  modern inform extract ie system implement sequ...   \n",
              "9993  document contain inform use variou applic ques...   \n",
              "9994  develop calm coordin analyz improv upon conjun...   \n",
              "9995  goal open inform extract oie extract surfac re...   \n",
              "9996  substanti progress factoid questionansw qa ans...   \n",
              "\n",
              "                                             lemmatized  \n",
              "0     present fashionmnist new dataset comprising x ...  \n",
              "1     tensorflow machine learning system operates la...  \n",
              "2     tensorflow interface expressing machine learni...  \n",
              "3     goal precipitation nowcasting predict future r...  \n",
              "4     machine learning address question build comput...  \n",
              "...                                                 ...  \n",
              "9992  modern information extraction ie system implem...  \n",
              "9993  document contain information used various appl...  \n",
              "9994  develop calm coordination analyzer improves up...  \n",
              "9995  goal open information extraction oie extract s...  \n",
              "9996  substantial progress factoid questionanswering...  \n",
              "\n",
              "[9997 rows x 7 columns]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "a-06lTXjUqzz",
        "outputId": "930f7bc4-6fc8-4387-cd78-f3c1ff31bf15"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7a01ab80-56dc-40ae-a200-c94b4d46af91\", \"df_clean.csv\", 64400856)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_67b75f68-55cb-49c3-a40f-21078d4088f9\", \"df_cleaned_data.csv\", 8240336)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "df_cleaned_data = df_clean['lemmatized']\n",
        "df_cleaned_data\n",
        "\n",
        "df_clean.to_csv('df_clean.csv', index=False)\n",
        "df_cleaned_data.to_csv('df_cleaned_data.csv', index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"df_clean.csv\")\n",
        "files.download(\"df_cleaned_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y0oOSlsOS0cq",
        "outputId": "c2f32379-f911-499d-daca-28b3c7f40eb5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constituency Parsing Tree:\n",
            "present amod image NOUN []\n",
            "fashionmnist amod image NOUN []\n",
            "new amod dataset NOUN []\n",
            "dataset nmod product NOUN [new, comprising]\n",
            "comprising acl dataset NOUN []\n",
            "x punct product NOUN []\n",
            "grayscale compound image NOUN []\n",
            "image compound product NOUN [grayscale]\n",
            "fashion compound product NOUN []\n",
            "product compound image NOUN [dataset, x, image, fashion]\n",
            "category compound image NOUN []\n",
            "image nsubj serve VERB [present, fashionmnist, product, category, per]\n",
            "per prep image NOUN [fashionmnist]\n",
            "category compound training NOUN []\n",
            "training compound set VERB [category]\n",
            "set amod test NOUN []\n",
            "image compound test NOUN []\n",
            "test compound set VERB [set, image]\n",
            "set amod fashionmnist NOUN [training, test]\n",
            "image compound fashionmnist NOUN []\n",
            "fashionmnist pobj per ADP [set, image, intended]\n",
            "intended acl fashionmnist NOUN []\n",
            "serve ROOT serve VERB [image, learning, dataset, url]\n",
            "direct amod replacement NOUN []\n",
            "dropin nmod replacement NOUN []\n",
            "replacement nmod learning VERB [direct, dropin]\n",
            "original amod learning VERB []\n",
            "mnist amod learning VERB []\n",
            "dataset amod machine NOUN []\n",
            "benchmarking amod machine NOUN []\n",
            "machine compound learning VERB [dataset, benchmarking]\n",
            "learning dobj serve VERB [replacement, original, mnist, machine, split]\n",
            "algorithm compound share VERB []\n",
            "share compound testing NOUN [algorithm]\n",
            "image compound size NOUN []\n",
            "size compound structure NOUN [image]\n",
            "data compound format NOUN []\n",
            "format compound structure NOUN [data]\n",
            "structure compound testing NOUN [size, format]\n",
            "training compound testing NOUN []\n",
            "testing nsubj split VERB [share, structure, training]\n",
            "split relcl learning VERB [testing]\n",
            "dataset dep serve VERB []\n",
            "freely advmod available ADJ []\n",
            "available amod http NOUN [freely]\n",
            "http compound url NOUN [available]\n",
            "url dobj serve VERB [http]\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "present fashionmnist new dataset comprising x grayscale image fashion product category image per category training set image test set image fashionmnist intended serve direct dropin replacement original mnist dataset benchmarking machine learning algorithm share image size data format structure training testing split dataset freely available http url\n",
            "==================================================\n",
            "Constituency Parsing Tree:\n",
            "tensorflow amod system NOUN []\n",
            "machine compound learning NOUN []\n",
            "learning compound system NOUN [machine]\n",
            "system nsubj operates VERB [tensorflow, learning]\n",
            "operates ROOT operates VERB [system, environment, tensorflow, customdesigned, give, describe, demonstrate]\n",
            "large amod scale NOUN []\n",
            "scale nmod environment NOUN [large]\n",
            "heterogeneous amod environment NOUN []\n",
            "environment dobj operates VERB [scale, heterogeneous]\n",
            "tensorflow conj operates VERB [us, dataflow, graph, across]\n",
            "us dobj tensorflow VERB []\n",
            "dataflow dobj tensorflow VERB []\n",
            "graph nmod computation NOUN []\n",
            "represent amod computation NOUN []\n",
            "computation npadvmod shared VERB [graph, represent]\n",
            "shared amod map NOUN [computation]\n",
            "state compound operation NOUN []\n",
            "operation npadvmod mutate ADJ [state]\n",
            "mutate amod map NOUN [operation]\n",
            "state compound map NOUN []\n",
            "map compound graph NOUN [shared, mutate, state]\n",
            "node compound dataflow PROPN []\n",
            "dataflow compound graph NOUN [node]\n",
            "graph dobj tensorflow VERB [map, dataflow, across]\n",
            "across prep graph NOUN [cluster]\n",
            "many amod cluster NOUN []\n",
            "machine compound cluster NOUN []\n",
            "cluster pobj across ADP [many, machine, within]\n",
            "within prep cluster NOUN [machine]\n",
            "machine pobj within ADP []\n",
            "across prep tensorflow VERB [device]\n",
            "multiple amod device NOUN []\n",
            "computational amod device NOUN []\n",
            "device pobj across ADP [multiple, computational, including]\n",
            "including prep device NOUN [gpus]\n",
            "multicore amod gpus NOUN []\n",
            "cpu compound generalpurpose NOUN []\n",
            "generalpurpose compound gpus NOUN [cpu]\n",
            "gpus pobj including VERB [multicore, generalpurpose]\n",
            "customdesigned advcl operates VERB []\n",
            "asics npadvmod known VERB []\n",
            "known amod architecture NOUN [asics]\n",
            "tensor compound processing NOUN []\n",
            "processing compound unit NOUN [tensor]\n",
            "unit nmod architecture NOUN [processing]\n",
            "tpus compound architecture NOUN []\n",
            "architecture nsubj give VERB [known, unit, tpus]\n",
            "give conj operates VERB [architecture, developer, shared, become]\n",
            "flexibility compound developer NOUN []\n",
            "application compound developer NOUN []\n",
            "developer dobj give VERB [flexibility, application]\n",
            "whereas mark shared VERB []\n",
            "previous amod management NOUN []\n",
            "parameter compound server NOUN []\n",
            "server compound management NOUN [parameter]\n",
            "design compound management NOUN []\n",
            "management nsubj shared VERB [previous, server, design]\n",
            "shared advcl give VERB [whereas, management, enables]\n",
            "state npadvmod built VERB []\n",
            "built amod tensorflow NOUN [state]\n",
            "system compound tensorflow NOUN []\n",
            "tensorflow nsubj enables VERB [built, system]\n",
            "enables ccomp shared VERB [tensorflow, tensorflow]\n",
            "developer compound optimization NOUN []\n",
            "experiment compound optimization NOUN []\n",
            "novel compound optimization NOUN []\n",
            "optimization compound training NOUN [developer, experiment, novel]\n",
            "training nsubj tensorflow VERB [optimization]\n",
            "algorithm nsubj tensorflow VERB []\n",
            "tensorflow ccomp enables VERB [training, algorithm, inference]\n",
            "support compound focus NOUN []\n",
            "variety compound application NOUN []\n",
            "application compound focus NOUN [variety]\n",
            "focus compound training NOUN [support, application]\n",
            "training compound inference NOUN [focus]\n",
            "inference dobj tensorflow VERB [training]\n",
            "deep amod network NOUN []\n",
            "neural amod network NOUN []\n",
            "network nsubj become VERB [deep, neural, use, released]\n",
            "several amod service NOUN []\n",
            "google compound service NOUN []\n",
            "service nsubj use VERB [several, google]\n",
            "use relcl network NOUN [service, production]\n",
            "tensorflow compound production NOUN []\n",
            "production dobj use VERB [tensorflow]\n",
            "released acl network NOUN [project]\n",
            "opensource amod project NOUN []\n",
            "project dobj released VERB [opensource]\n",
            "become conj give VERB [network, paper]\n",
            "widely advmod used VERB []\n",
            "used amod paper NOUN [widely]\n",
            "machine compound learning VERB []\n",
            "learning compound paper NOUN [machine]\n",
            "research compound paper NOUN []\n",
            "paper attr become VERB [used, learning, research]\n",
            "describe advcl operates VERB []\n",
            "tensorflow amod model NOUN []\n",
            "dataflow amod model NOUN []\n",
            "model nsubj demonstrate VERB [tensorflow, dataflow]\n",
            "demonstrate conj operates VERB [model, achieves]\n",
            "compelling amod performance NOUN []\n",
            "performance compound tensorflow NOUN [compelling]\n",
            "tensorflow nsubj achieves VERB [performance]\n",
            "achieves ccomp demonstrate VERB [tensorflow, application]\n",
            "several amod application NOUN []\n",
            "realworld compound application NOUN []\n",
            "application dobj achieves VERB [several, realworld]\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "tensorflow machine learning system operates large scale heterogeneous environment tensorflow us dataflow graph represent computation shared state operation mutate state map node dataflow graph across many machine cluster within machine across multiple computational device including multicore cpu generalpurpose gpus customdesigned asics known tensor processing unit tpus architecture give flexibility application developer whereas previous parameter server design management shared state built system tensorflow enables developer experiment novel optimization training algorithm tensorflow support variety application focus training inference deep neural network several google service use tensorflow production released opensource project become widely used machine learning research paper describe tensorflow dataflow model demonstrate compelling performance tensorflow achieves several realworld application\n",
            "==================================================\n",
            "                                             lemmatized  noun  verb  adjective  adverb  \\\n",
            "0     present fashionmnist new dataset comprising x ...    28     8          9       1   \n",
            "1     tensorflow machine learning system operates la...    60    17         23       1   \n",
            "2     tensorflow interface expressing machine learni...    56    18         22       1   \n",
            "3     goal precipitation nowcasting predict future r...    42    11         19       4   \n",
            "4     machine learning address question build comput...    32    19         10       2   \n",
            "...                                                 ...   ...   ...        ...     ...   \n",
            "9992  modern information extraction ie system implem...    36    11         17       4   \n",
            "9993  document contain information used various appl...    56    17         27       3   \n",
            "9994  develop calm coordination analyzer improves up...    39    12         22       0   \n",
            "9995  goal open information extraction oie extract s...    38     9         17       4   \n",
            "9996  substantial progress factoid questionanswering...    38    21         20       9   \n",
            "\n",
            "                                 entities  \n",
            "0                                      []  \n",
            "1                                      []  \n",
            "2     [dozen - CARDINAL, november - DATE]  \n",
            "3                                      []  \n",
            "4                          [today - DATE]  \n",
            "...                                   ...  \n",
            "9992                   [three - CARDINAL]  \n",
            "9993                                   []  \n",
            "9994                                   []  \n",
            "9995                                   []  \n",
            "9996                                   []  \n",
            "\n",
            "[9997 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import spacy\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load SpaCy English model for Named Entity Recognition\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the CSV file containing the clean text\n",
        "try:\n",
        "    df = pd.read_csv('df_cleaned_data.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please ensure the file path is correct.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(\"An error occurred while reading the CSV file:\", e)\n",
        "    exit()\n",
        "\n",
        "# Function to conduct Parts of Speech (POS) Tagging and calculate counts\n",
        "def pos_tagging_and_count(text):\n",
        "    nouns = verbs = adjectives = adverbs = 0\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        tokens = word_tokenize(text)\n",
        "        pos_tags = pos_tag(tokens)\n",
        "        for _, tag in pos_tags:\n",
        "            if tag.startswith('N'):  # Noun\n",
        "                nouns += 1\n",
        "            elif tag.startswith('V'):  # Verb\n",
        "                verbs += 1\n",
        "            elif tag.startswith('J'):  # Adjective\n",
        "                adjectives += 1\n",
        "            elif tag.startswith('R'):  # Adverb\n",
        "                adverbs += 1\n",
        "    return nouns, verbs, adjectives, adverbs\n",
        "\n",
        "# Function to perform Constituency Parsing and Dependency Parsing\n",
        "def parse_trees(text):\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        sentences = sent_tokenize(text)\n",
        "        for sentence in sentences:\n",
        "            print(\"Constituency Parsing Tree:\")\n",
        "            parsed_sentence = nlp(sentence)\n",
        "            for token in parsed_sentence:\n",
        "                print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "                      [child for child in token.children])\n",
        "            print(\"\\nDependency Parsing Tree:\")\n",
        "            print(parsed_sentence)\n",
        "            print(\"=\"*50)\n",
        "\n",
        "# Function to perform Named Entity Recognition (NER) and calculate counts\n",
        "def ner_and_count(text):\n",
        "    entities = []\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        doc = nlp(text)\n",
        "        for ent in doc.ents:\n",
        "            entities.append(ent.text + \" - \" + ent.label_)\n",
        "    return entities\n",
        "\n",
        "# Apply functions to each row of the DataFrame\n",
        "df['noun'], df['verb'], df['adjective'], df['adverb'] = zip(*df['lemmatized'].apply(pos_tagging_and_count))\n",
        "\n",
        "df['lemmatized'].head(2).apply(parse_trees)\n",
        "#df['lemmatized'].apply(parse_trees)\n",
        "df['entities'] = df['lemmatized'].apply(ner_and_count)\n",
        "\n",
        "# Display DataFrame with added columns\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "_e557s2w4BpK",
        "outputId": "45a32e2e-13b1-4fb8-9e4c-1bca892724a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nit was bit of tough thing to do as the data scrapping is not working good at all, seroiusly i tried each and every topic to do. \\ni am not happy with kind of assignment may be it was easy when we learned completely but when we are still in learning state not easy.\\nand the last step the 3rd question challaged in many ways i did understand the concepts now thanks to you.\\ncannot complain about the time but yeah doing this with out any help from the experts not going in my way, \\ni would like to understand more about the how to collect data throught hyper links in a hyperlink. \\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# Write your response below\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "it was bit of tough thing to do as the data scrapping is not working good at all, seroiusly i tried each and every topic to do.\n",
        "i am not happy with kind of assignment may be it was easy when we learned completely but when we are still in learning state not easy.\n",
        "and the last step the 3rd question challaged in many ways i did understand the concepts now thanks to you.\n",
        "cannot complain about the time but yeah doing this with out any help from the experts not going in my way,\n",
        "i would like to understand more about the how to collect data throught hyper links in a hyperlink.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2TVQ7KCSo3oS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}