{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshaSolingaram/INFO_5731/blob/main/Solingaram_Harshavardhan_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Here are five types of features that could be useful for building a machine learning model for sentiment analysis:\n",
        "\n",
        "Bag of Words (BoW) Features: BoW features represent the presence or absence of words in the text.\n",
        "These features can capture the frequency of specific words and their importance in determining sentiment.\n",
        "Words like \"excellent,\" \"terrible,\" \"satisfied,\" and \"disappointed\" are likely to be indicative of sentiment.\n",
        "\n",
        "TF-IDF Features: Term Frequency-Inverse Document Frequency (TF-IDF) features weigh the importance of a word in a document relative to a corpus of documents.\n",
        "This feature type helps in identifying words that are frequent in a document but rare in the overall corpus, which can be strong indicators of sentiment.\n",
        "\n",
        "N-grams Features: N-grams features capture sequences of adjacent words in the text.\n",
        "Unigrams (single words), bigrams (pairs of adjacent words), and trigrams (triplets of adjacent words) can provide context and capture nuances in sentiment that may be missed by individual words alone.\n",
        "\n",
        "Part-of-Speech (POS) Features: POS features categorize words in a text into their grammatical categories (e.g., nouns, verbs, adjectives).\n",
        "Adjectives and adverbs often carry sentiment information, so including features based on POS tagging can help capture the sentiment expressed in the text more accurately.\n",
        "\n",
        "Sentiment Lexicon Features: Sentiment lexicons contain lists of words annotated with their associated sentiment polarity (positive, negative, or neutral).\n",
        "Incorporating features based on sentiment lexicons allows the model to leverage pre-defined sentiment knowledge, enhancing its ability to recognize sentiment in text.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87563e64-74f0-4be4-938a-145c1be60d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  \\\n",
            "0   The product is excellent and highly recommended.   \n",
            "1  I'm really disappointed with the service provi...   \n",
            "2          Overall, I am satisfied with my purchase.   \n",
            "3     The quality of the item is terrible, avoid it!   \n",
            "4  This company offers outstanding customer support.   \n",
            "\n",
            "                                        BoW Features  \\\n",
            "0  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
            "1  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, ...   \n",
            "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...   \n",
            "3  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
            "4  [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                     TF-IDF Features  \\\n",
            "0  [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, ...   \n",
            "1  [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "3  [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, ...   \n",
            "4  [0.0, 0.4472135954999579, 0.4472135954999579, ...   \n",
            "\n",
            "                                    N-grams Features    POS Features  \\\n",
            "0  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, ...    NN JJ RB VBD   \n",
            "1  [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...    RB JJ NN VBD   \n",
            "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       JJ VBD NN   \n",
            "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...     NN NN JJ NN   \n",
            "4  [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  NN NN JJ NN NN   \n",
            "\n",
            "  Lexicon Features Sentiment  \n",
            "0         positive  positive  \n",
            "1         negative  negative  \n",
            "2         positive  positive  \n",
            "3         negative  negative  \n",
            "4         positive  positive  \n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample text data\n",
        "reviews = [\n",
        "    \"The product is excellent and highly recommended.\",\n",
        "    \"I'm really disappointed with the service provided.\",\n",
        "    \"Overall, I am satisfied with my purchase.\",\n",
        "    \"The quality of the item is terrible, avoid it!\",\n",
        "    \"This company offers outstanding customer support.\"\n",
        "]\n",
        "labels = ['positive', 'negative', 'positive', 'negative', 'positive']\n",
        "\n",
        "# Tokenization, stop words removal, and lemmatization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "preprocessed_reviews = [preprocess_text(review) for review in reviews]\n",
        "\n",
        "# Bag of Words (BoW) Features\n",
        "vectorizer = CountVectorizer()\n",
        "bow_features = vectorizer.fit_transform(preprocessed_reviews)\n",
        "\n",
        "# TF-IDF Features\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(preprocessed_reviews)\n",
        "\n",
        "# N-grams Features (bigrams)\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "ngram_features = ngram_vectorizer.fit_transform(preprocessed_reviews)\n",
        "\n",
        "# Part-of-Speech (POS) Features\n",
        "pos_features = []\n",
        "for review in preprocessed_reviews:\n",
        "    tokens = nltk.pos_tag(word_tokenize(review))\n",
        "    pos_tags = [tag for word, tag in tokens]\n",
        "    pos_features.append(' '.join(pos_tags))\n",
        "\n",
        "# Sentiment Lexicon Features\n",
        "positive_lexicon = set(['excellent', 'recommended', 'satisfied', 'outstanding'])\n",
        "negative_lexicon = set(['disappointed', 'terrible', 'avoid'])\n",
        "lexicon_features = []\n",
        "for review in preprocessed_reviews:\n",
        "    pos_words = set(word_tokenize(review)).intersection(positive_lexicon)\n",
        "    neg_words = set(word_tokenize(review)).intersection(negative_lexicon)\n",
        "    if len(pos_words) > len(neg_words):\n",
        "        lexicon_features.append('positive')\n",
        "    elif len(pos_words) < len(neg_words):\n",
        "        lexicon_features.append('negative')\n",
        "    else:\n",
        "        lexicon_features.append('neutral')\n",
        "\n",
        "# Combine features into a DataFrame\n",
        "features_df = pd.DataFrame({\n",
        "    'Review': reviews,\n",
        "    'BoW Features': [f.toarray()[0] for f in bow_features],\n",
        "    'TF-IDF Features': [f.toarray()[0] for f in tfidf_features],\n",
        "    'N-grams Features': [f.toarray()[0] for f in ngram_features],\n",
        "    'POS Features': pos_features,\n",
        "    'Lexicon Features': lexicon_features,\n",
        "    'Sentiment': labels\n",
        "})\n",
        "\n",
        "print(features_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ded4739-2e19-4ba9-dce8-bf341035e722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked features based on Mutual Information scores:\n",
            "Feature: avoid, MI Score: 0.22314355131420974\n",
            "Feature: disappointed, MI Score: 0.22314355131420974\n",
            "Feature: item, MI Score: 0.22314355131420974\n",
            "Feature: provided, MI Score: 0.22314355131420974\n",
            "Feature: quality, MI Score: 0.22314355131420974\n",
            "Feature: really, MI Score: 0.22314355131420974\n",
            "Feature: service, MI Score: 0.22314355131420974\n",
            "Feature: terrible, MI Score: 0.22314355131420974\n",
            "Feature: company, MI Score: 0.11849392256130009\n",
            "Feature: customer, MI Score: 0.11849392256130009\n",
            "Feature: excellent, MI Score: 0.11849392256130009\n",
            "Feature: highly, MI Score: 0.11849392256130009\n",
            "Feature: offer, MI Score: 0.11849392256130009\n",
            "Feature: outstanding, MI Score: 0.11849392256130009\n",
            "Feature: overall, MI Score: 0.11849392256130009\n",
            "Feature: product, MI Score: 0.11849392256130009\n",
            "Feature: purchase, MI Score: 0.11849392256130009\n",
            "Feature: recommended, MI Score: 0.11849392256130009\n",
            "Feature: satisfied, MI Score: 0.11849392256130009\n",
            "Feature: support, MI Score: 0.11849392256130009\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Convert labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Calculate Mutual Information scores for each feature\n",
        "mi_scores = mutual_info_classif(bow_features, encoded_labels)\n",
        "\n",
        "# Map feature indices to their corresponding Mutual Information scores\n",
        "feature_mi_scores = dict(zip(range(len(vectorizer.vocabulary_)), mi_scores))\n",
        "\n",
        "# Sort features based on their Mutual Information scores in descending order\n",
        "sorted_features = sorted(feature_mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display sorted features and their Mutual Information scores\n",
        "print(\"Ranked features based on Mutual Information scores:\")\n",
        "for feature_idx, score in sorted_features:\n",
        "    feature_name = list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(feature_idx)]\n",
        "    print(f\"Feature: {feature_name}, MI Score: {score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f24727b-e281-4278-c64c-9084df2f8388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked results based on text similarity:\n",
            "Rank 1: Similarity: 0.9329 - Text: I'm really disappointed with the service provided.\n",
            "Rank 2: Similarity: 0.9194 - Text: Overall, I am satisfied with my purchase.\n",
            "Rank 3: Similarity: 0.8883 - Text: The product is excellent and highly recommended.\n",
            "Rank 4: Similarity: 0.8809 - Text: This company offers outstanding customer support.\n",
            "Rank 5: Similarity: 0.8237 - Text: The quality of the item is terrible, avoid it!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample text data\n",
        "reviews = [\n",
        "    \"The product is excellent and highly recommended.\",\n",
        "    \"I'm really disappointed with the service provided.\",\n",
        "    \"Overall, I am satisfied with my purchase.\",\n",
        "    \"The quality of the item is terrible, avoid it!\",\n",
        "    \"This company offers outstanding customer support.\"\n",
        "]\n",
        "\n",
        "# Query\n",
        "query = \"I want to buy a product and I'm looking for excellent recommendations.\"\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize query and texts, then convert them to tensors\n",
        "query_tokens = tokenizer.encode_plus(query, add_special_tokens=True, max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n",
        "text_tokens = [tokenizer.encode_plus(text, add_special_tokens=True, max_length=512, truncation=True, padding='max_length', return_tensors='pt') for text in reviews]\n",
        "\n",
        "# Calculate BERT embeddings for query and texts\n",
        "with torch.no_grad():\n",
        "    query_outputs = model(**query_tokens)\n",
        "    text_outputs = [model(**text_token) for text_token in text_tokens]\n",
        "\n",
        "query_embeddings = query_outputs.last_hidden_state[:, 0, :].numpy().reshape(1, -1)  # Reshape to 2D array\n",
        "text_embeddings = [output.last_hidden_state[:, 0, :].numpy().reshape(1, -1) for output in text_outputs]\n",
        "\n",
        "# Calculate cosine similarity between query and each text\n",
        "similarities = [cosine_similarity(query_embeddings, text_embedding)[0][0] for text_embedding in text_embeddings]\n",
        "\n",
        "# Rank texts based on similarity in descending order\n",
        "ranked_results = sorted(zip(reviews, similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print ranked results\n",
        "print(\"Ranked results based on text similarity:\")\n",
        "for i, (text, similarity) in enumerate(ranked_results):\n",
        "    print(f\"Rank {i+1}: Similarity: {similarity:.4f} - Text: {text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "\n",
        "i would like to get clarification about the exercies, quiz, and assignments.\n",
        "beacause we are now learning from scrath, we need help and some hands on to do this assignments.\n",
        "the time oyu guys given for this exercise i am a little bit dissapoited.\n",
        "the exercise is obviously helpul please add some time for the next time.\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cbce5c5a-fe1f-430e-823c-57756c50caf5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\n\\n\\ni would like to get clarification about the exercies, quiz, and assignments. \\nbeacause we are now learning from scrath, we need help and some hands on to do this assignments. \\nthe time oyu guys given for this exercise i am a little bit dissapoited.\\nthe exercise is obviously helpul please add some time for the next time.\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}