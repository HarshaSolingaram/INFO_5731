{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshaSolingaram/INFO_5731/blob/main/Solingaram_Harshavardhan_Exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Prepare the text corpus\n",
        "\n",
        "textdatas = [\"I watched the movie first and then I discovered that there was book to the movie and those who know me know that I cant pass up a book/movie combo\",\n",
        "             \"This book was totally unlike the movie I personally like the book more than the movie and even though the movie was good\",\n",
        "             \"the book was so full of emotion it connects you with what is going on in Tessa's mind and it makes you want more and to know what is going to happen\",\n",
        "             \"being almost 16 feel like people sugar coat what goes on in their first real relationship\",\n",
        "             \"Anna Todd if your reading this thank you for writing such an amazing book. To sum this up I definitely recommend reading this book\"]\n",
        "\n",
        "# Tokenize the texts\n",
        "tokenized_texts = [textdata.lower().split() for textdata in textdatas]\n",
        "\n",
        "# Create a dictionary from the tokenized texts\n",
        "dictionary = corpora.Dictionary(tokenized_texts)\n",
        "\n",
        "# Convert the tokenized texts into a bag-of-words representation\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
        "\n",
        "# Find the optimal number of topics using coherence score\n",
        "coherence_scores = {}\n",
        "for k in range(2, 11):\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=k,\n",
        "                                                random_state=100, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=tokenized_texts, dictionary=dictionary, coherence='c_v')\n",
        "\n",
        "    coherence_scores[k] = coherence_model.get_coherence()\n",
        "    print(f\"coherence_scores[{k+1}]:\",coherence_scores[k])\n",
        "\n",
        "optimal_k = max(coherence_scores, key=coherence_scores.get)\n",
        "\n",
        "print(f\"The optimal number of topics is {optimal_k}\")\n",
        "\n",
        "# Train the LDA model with the optimal number of topics\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=optimal_k,\n",
        "                                            random_state=100, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
        "\n",
        "# Summarize the topics\n",
        "topics = lda_model.show_topics(formatted=False)\n",
        "for i, topic in enumerate(topics):\n",
        "    print(f\"Topic {i+1}: {' '.join([w[0] for w in topic[1]])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO2ICuV-5VMn",
        "outputId": "43b2be8b-caf4-4c94-b242-ebe8f401a352"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coherence_scores[3]: 0.36765756896062923\n",
            "coherence_scores[4]: 0.736029644819416\n",
            "coherence_scores[5]: 0.6201629151127688\n",
            "coherence_scores[6]: 0.618340544697902\n",
            "coherence_scores[7]: 0.8070711526499893\n",
            "coherence_scores[8]: 0.765308914569904\n",
            "coherence_scores[9]: 0.5830118627211708\n",
            "coherence_scores[10]: 0.6901764547444075\n",
            "coherence_scores[11]: 0.6744320334732778\n",
            "The optimal number of topics is 6\n",
            "Topic 1: you it going and is to what so was full\n",
            "Topic 2: what to going is and you it of makes book\n",
            "Topic 3: the movie i and was book know that to more\n",
            "Topic 4: to you it what is and with going on in\n",
            "Topic 5: in their goes being real sugar coat feel on 16\n",
            "Topic 6: this reading todd recommend if anna thank to up you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1255d47-b6c1-4d31-dd00-e3e067dd9a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score:  0.6434173301145665\n",
            "Topic 1: ['the', 'movie', 'and', 'I', 'book', 'was', 'know', 'to', 'that', 'more']\n",
            "Topic 2: ['what', 'it', 'is', 'going', 'you', 'movie', 'to', 'I', 'the', 'on']\n",
            "Topic 3: ['this', 'reading', 'up', 'definitely', 'an', 'book.', 'your', 'Anna', 'To', 'Todd']\n",
            "Topic 4: ['that', 'know', 'the', 'I', 'book', 'there', 'book/movie', 'pass', 'watched', 'then']\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "from gensim.models import LsiModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "textdatas = [\"I watched the movie first and then I discovered that there was book to the movie and those who know me know that I cant pass up a book/movie combo\",\n",
        "             \"This book was totally unlike the movie I personally like the book more than the movie and even though the movie was good\",\n",
        "             \"the book was so full of emotion it connects you with what is going on in Tessa's mind and it makes you want more and to know what is going to happen\",\n",
        "             \"being almost 16 feel like people sugar coat what goes on in their first real relationship\",\n",
        "             \"Anna Todd if your reading this thank you for writing such an amazing book. To sum this up I definitely recommend reading this book\"]\n",
        "\n",
        "\n",
        "# Create dictionary\n",
        "texts_tokenized = [text.split() for text in textdatas]\n",
        "dictionary = Dictionary(texts_tokenized)\n",
        "\n",
        "# Create corpus\n",
        "corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n",
        "\n",
        "# Build LSA model\n",
        "num_topics = 4\n",
        "lsa_model = LsiModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
        "\n",
        "# Compute coherence score\n",
        "coherence_model_lsa = CoherenceModel(model=lsa_model, texts=texts_tokenized, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lsa = coherence_model_lsa.get_coherence()\n",
        "\n",
        "# Print coherence score\n",
        "print('Coherence Score: ', coherence_lsa)\n",
        "\n",
        "# Print topics\n",
        "topics = lsa_model.show_topics(formatted=False)\n",
        "for i, topic in enumerate(topics):\n",
        "    print('Topic {}: {}'.format(i+1, [word[0] for word in topic[1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de8b72fa-6d54-4a4a-eaca-8f7526f6580d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lda2vec in /usr/local/lib/python3.10/dist-packages (0.16.10)\n",
            "Topic 1: book, know, movi, read, go, anna, amaz, write, todd, thank\n",
            "Topic 2: like, relationship, peopl, coat, feel, goe, sugar, real, book, movi\n",
            "Topic 3: go, emot, mind, make, want, happen, tessa, connect, book, know\n",
            "Topic 4: movi, book, like, know, go, make, mind, connect, happen, read\n",
            "Topic 5: movi, book, person, total, like, good, unlik, know, go, happen\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "# Write your code here\n",
        "!pip install lda2vec\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "textdatas = [\"I watched the movie first and then I discovered that there was book to the movie and those who know me know that I cant pass up a book/movie combo\",\n",
        "             \"This book was totally unlike the movie I personally like the book more than the movie and even though the movie was good\",\n",
        "             \"the book was so full of emotion it connects you with what is going on in Tessa's mind and it makes you want more and to know what is going to happen\",\n",
        "             \"being almost 16 feel like people sugar coat what goes on in their first real relationship\",\n",
        "             \"Anna Todd if your reading this thank you for writing such an amazing book. To sum this up I definitely recommend reading this book\"]\n",
        "\n",
        "\n",
        "# Preprocess the text data\n",
        "processed_texts = [preprocess_string(text) for text in textdatas]\n",
        "\n",
        "# Create a dictionary and corpus\n",
        "dictionary = gensim.corpora.Dictionary(processed_texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
        "\n",
        "# Train the LDA model\n",
        "vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "doc_term_matrix = vectorizer.fit_transform(textdatas)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=10)\n",
        "\n",
        "# Get the topics and their top words\n",
        "topics = lda_model.show_topics(num_topics=5, num_words=10, formatted=False)\n",
        "top_words_per_topic = []\n",
        "for topic in topics:\n",
        "    top_words = [word[0] for word in topic[1]]\n",
        "    top_words_per_topic.append(top_words)\n",
        "\n",
        "# Print the top words for each topic\n",
        "for i, top_words in enumerate(top_words_per_topic):\n",
        "    print(f\"Topic {i+1}: {', '.join(top_words)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install bertopic"
      ],
      "metadata": {
        "id": "2NeuYxxiy_sp"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "textdatas = [\"I watched the movie first and then I discovered that there was book to the movie and those who know me know that I cant pass up a book/movie combo\",\n",
        "             \"This book was totally unlike the movie I personally like the book more than the movie and even though the movie was good\",\n",
        "             \"the book was so full of emotion it connects you with what is going on in Tessa's mind and it makes you want more and to know what is going to happen\",\n",
        "             \"being almost 16 feel like people sugar coat what goes on in their first real relationship\",\n",
        "             \"Anna Todd if your reading this thank you for writing such an amazing book. To sum this up I definitely recommend reading this book\"]\n",
        "\n",
        "\n",
        "# Initialize BERTopic model\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
        "\n",
        "# Fit the model to the data and transform it into topic representations\n",
        "topics, probabilities = topic_model.fit_transform(textdatas)\n",
        "\n",
        "# Print the topics\n",
        "print(\"\\nTopics:\")\n",
        "for topic_id in range(topic_model.get_params()[\"n_components\"]):\n",
        "    topic_words = topic_model.get_topic(topic_id)\n",
        "    print(f\"Topic {topic_id + 1}: {', '.join(topic_words)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599,
          "referenced_widgets": [
            "81b831c7ad3944c7bf2f75833abda930",
            "4d2080e69a8a4f2b8cfb81e3903b69db",
            "18929e7c66024b459c40c9a3eeecabec",
            "1e4cefcf7c5942b9aa75bcb3548f96a5",
            "4b9dd93cf5f3450b93cd7a355ada05f6",
            "b2b36828d42a4cd9986945caefdf7887",
            "722b6c7be4f04a72b2ece0c963ae6d3e",
            "1dc422b787494c8b9e1347464e8007ad",
            "faf211dec0d642ed9b37d75248283dee",
            "bd0785618f4c4c3597e6e4ff98869cf2",
            "ee441fe931154e53ad2ddae838754ed0"
          ]
        },
        "id": "IijoLj2JDM5p",
        "outputId": "cc171086-97a5-4722-b1b6-b25bf4eb599d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-30 04:29:27,557 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81b831c7ad3944c7bf2f75833abda930"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-30 04:29:28,976 - BERTopic - Embedding - Completed ✓\n",
            "2024-03-30 04:29:28,978 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
            "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
            "  warnings.warn(\"k >= N for N * N square matrix. \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36m_reduce_dimensionality\u001b[0;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[1;32m   3354\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3355\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3356\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2779\u001b[0m             )\n\u001b[0;32m-> 2780\u001b[0;31m             self.embedding_, aux_data = self._fit_embed_data(\n\u001b[0m\u001b[1;32m   2781\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36m_fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2825\u001b[0m         \"\"\"\n\u001b[0;32m-> 2826\u001b[0;31m         return simplicial_set_embedding(\n\u001b[0m\u001b[1;32m   2827\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spectral\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         embedding = spectral_layout(\n\u001b[0m\u001b[1;32m   1107\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/spectral.py\u001b[0m in \u001b[0;36mspectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, tol, maxiter)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m     return _spectral_layout(\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/spectral.py\u001b[0m in \u001b[0;36m_spectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, init, method, tol, maxiter)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eigsh\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             eigenvalues, eigenvectors = scipy.sparse.linalg.eigsh(\n\u001b[0m\u001b[1;32m    522\u001b[0m                 \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py\u001b[0m in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m             raise TypeError(\"Cannot use scipy.linalg.eigh for sparse A with \"\n\u001b[0m\u001b[1;32m   1606\u001b[0m                             \u001b[0;34m\"k >= N. Use scipy.linalg.eigh(A.toarray()) or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-00e8e7884ad7>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Fit the model to the data and transform it into topic representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextdatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Print the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# Reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0mumap_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_dimensionality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;31m# Cluster reduced embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36m_reduce_dimensionality\u001b[0;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[1;32m   3356\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3358\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m         \u001b[0mumap_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2778\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m             )\n\u001b[0;32m-> 2780\u001b[0;31m             self.embedding_, aux_data = self._fit_embed_data(\n\u001b[0m\u001b[1;32m   2781\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2782\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36m_fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0mreplaced\u001b[0m \u001b[0mby\u001b[0m \u001b[0msubclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m         \"\"\"\n\u001b[0;32m-> 2826\u001b[0;31m         return simplicial_set_embedding(\n\u001b[0m\u001b[1;32m   2827\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spectral\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         embedding = spectral_layout(\n\u001b[0m\u001b[1;32m   1107\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/spectral.py\u001b[0m in \u001b[0;36mspectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, tol, maxiter)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mspectral\u001b[0m \u001b[0membedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m     return _spectral_layout(\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/spectral.py\u001b[0m in \u001b[0;36m_spectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, init, method, tol, maxiter)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eigsh\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             eigenvalues, eigenvectors = scipy.sparse.linalg.eigsh(\n\u001b[0m\u001b[1;32m    522\u001b[0m                 \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py\u001b[0m in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m             raise TypeError(\"Cannot use scipy.linalg.eigh for sparse A with \"\n\u001b[0m\u001b[1;32m   1606\u001b[0m                             \u001b[0;34m\"k >= N. Use scipy.linalg.eigh(A.toarray()) or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m                             \" reduce k.\")\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data we are providing for the bert is too small, its not supporting as it is underfitting. i have tried using with this and i came to know is bert requires more lines of data in thousands on the topic. so this coming an error due to underfitting.\n"
      ],
      "metadata": {
        "id": "kn8n1bEQiZWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 3 (Alternative) - (10 points)**\n",
        "\n",
        "If you are unable to do the topic modeling using lda2vec, do the alternate question.\n",
        "\n",
        "Provide atleast 3 visualization for the topics generated by the BERTopic or LDA model. Explain each of the visualization in detail."
      ],
      "metadata": {
        "id": "Wslk2SYHML8t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3wqPRYHpQqjL"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "k topics of LDA = 6\n",
        "k tpoics of LSA = 4\n",
        "k topics of lda2vec = 5\n",
        "\n",
        "as per the stats and researches BERTopic is the best model to work on topic modeling for large sets of data.\n",
        "but i have used a small data with 5 lines of code for this as per the coherence and k values\n",
        "i think Latent Dirichlet Allocation (LDA) is best fit model for my data. as it is genrated more and relevent data compared to other three.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OK34nZtojhmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0066405c-2324-404a-efea-71d10510dc38"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nk topics of LDA = 6\\nk tpoics of LSA = 4\\nk topics of lda2vec = 5\\n\\nas per the stats and researches BERTopic is the best model to work on topic modeling for large sets of data. \\nbut i have used a small data with 5 lines of code for this as per the coherence and k values \\ni think Latent Dirichlet Allocation (LDA) is best fit model for my data. as it is genrated more and relevent data compared to other three.\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "the exercise is good but i thought you would have tell how much input data should be provided.\n",
        "i have encountered issue with coherence in all the models. overall it is good.\n",
        "NLP is dragging me down to the bottom, iam losing my confidence in some cases but trying to keep with the course.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "91445a28-6196-4ce8-9861-15df26eb2bea"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\n\\nthe exercise is good but i thought you would have tell how much input data should be provided.\\ni have encountered issue with coherence in all the models. overall it is good.\\nNLP is dragging me down to the bottom, iam losing my confidence in some cases but trying to keep with the course.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "81b831c7ad3944c7bf2f75833abda930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d2080e69a8a4f2b8cfb81e3903b69db",
              "IPY_MODEL_18929e7c66024b459c40c9a3eeecabec",
              "IPY_MODEL_1e4cefcf7c5942b9aa75bcb3548f96a5"
            ],
            "layout": "IPY_MODEL_4b9dd93cf5f3450b93cd7a355ada05f6"
          }
        },
        "4d2080e69a8a4f2b8cfb81e3903b69db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2b36828d42a4cd9986945caefdf7887",
            "placeholder": "​",
            "style": "IPY_MODEL_722b6c7be4f04a72b2ece0c963ae6d3e",
            "value": "Batches: 100%"
          }
        },
        "18929e7c66024b459c40c9a3eeecabec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dc422b787494c8b9e1347464e8007ad",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_faf211dec0d642ed9b37d75248283dee",
            "value": 1
          }
        },
        "1e4cefcf7c5942b9aa75bcb3548f96a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd0785618f4c4c3597e6e4ff98869cf2",
            "placeholder": "​",
            "style": "IPY_MODEL_ee441fe931154e53ad2ddae838754ed0",
            "value": " 1/1 [00:00&lt;00:00,  4.71it/s]"
          }
        },
        "4b9dd93cf5f3450b93cd7a355ada05f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2b36828d42a4cd9986945caefdf7887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "722b6c7be4f04a72b2ece0c963ae6d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dc422b787494c8b9e1347464e8007ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faf211dec0d642ed9b37d75248283dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd0785618f4c4c3597e6e4ff98869cf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee441fe931154e53ad2ddae838754ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}